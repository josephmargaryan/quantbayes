{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa96225",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c8cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook Temperature Humidity    Wind PlayTennis\n",
      "0      Sunny         Hot     High    Weak         No\n",
      "1      Sunny         Hot     High  Strong         No\n",
      "2   Overcast         Hot     High    Weak        Yes\n",
      "3      Rainy        Mild     High    Weak        Yes\n",
      "4      Rainy        Cool   Normal    Weak        Yes\n",
      "5      Rainy        Cool   Normal  Strong         No\n",
      "6   Overcast        Cool   Normal  Strong        Yes\n",
      "7      Sunny        Mild     High    Weak         No\n",
      "8      Sunny        Cool   Normal    Weak        Yes\n",
      "9      Rainy        Mild   Normal    Weak        Yes\n",
      "10     Sunny        Mild   Normal  Strong        Yes\n",
      "11  Overcast        Mild     High  Strong        Yes\n",
      "12  Overcast         Hot   Normal    Weak        Yes\n",
      "13     Rainy        Mild     High  Strong         No\n",
      "Saved to play_tennis.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrame\n",
    "data = [\n",
    "    [\"Sunny\",    \"Hot\",  \"High\",   \"Weak\",   \"No\"],\n",
    "    [\"Sunny\",    \"Hot\",  \"High\",   \"Strong\", \"No\"],\n",
    "    [\"Overcast\", \"Hot\",  \"High\",   \"Weak\",   \"Yes\"],\n",
    "    [\"Rainy\",    \"Mild\", \"High\",   \"Weak\",   \"Yes\"],\n",
    "    [\"Rainy\",    \"Cool\", \"Normal\", \"Weak\",   \"Yes\"],\n",
    "    [\"Rainy\",    \"Cool\", \"Normal\", \"Strong\", \"No\"],\n",
    "    [\"Overcast\", \"Cool\", \"Normal\", \"Strong\", \"Yes\"],\n",
    "    [\"Sunny\",    \"Mild\", \"High\",   \"Weak\",   \"No\"],\n",
    "    [\"Sunny\",    \"Cool\", \"Normal\", \"Weak\",   \"Yes\"],\n",
    "    [\"Rainy\",    \"Mild\", \"Normal\", \"Weak\",   \"Yes\"],\n",
    "    [\"Sunny\",    \"Mild\", \"Normal\", \"Strong\", \"Yes\"],\n",
    "    [\"Overcast\", \"Mild\", \"High\",   \"Strong\", \"Yes\"],\n",
    "    [\"Overcast\", \"Hot\",  \"Normal\", \"Weak\",   \"Yes\"],\n",
    "    [\"Rainy\",    \"Mild\", \"High\",   \"Strong\", \"No\"],\n",
    "]\n",
    "\n",
    "columns = [\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\", \"PlayTennis\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save as CSV (no index column)\n",
    "# df.to_csv(\"play_tennis.csv\", index=False)\n",
    "print(\"Saved to play_tennis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e633da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with N=14 rows, 4 features. Label column: PlayTennis\n",
      "Feature names: ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
      "Class labels: ['No', 'Yes']\n",
      "\n",
      "=== Slide-style intermediate probabilities (no smoothing) ===\n",
      "x* = ['Sunny', 'Cool', 'High', 'Strong']\n",
      "\n",
      "P(Yes) = 9/14 = 0.642857\n",
      "P(No) = 5/14 = 0.357143\n",
      "\n",
      "--- Conditionals given Yes ---\n",
      "P(Sunny | Yes)  [feature=Outlook] = 2/9 = 0.222222\n",
      "P(Cool | Yes)  [feature=Temperature] = 3/9 = 0.333333\n",
      "P(High | Yes)  [feature=Humidity] = 3/9 = 0.333333\n",
      "P(Strong | Yes)  [feature=Wind] = 3/9 = 0.333333\n",
      "\n",
      "--- Conditionals given No ---\n",
      "P(Sunny | No)  [feature=Outlook] = 3/5 = 0.600000\n",
      "P(Cool | No)  [feature=Temperature] = 1/5 = 0.200000\n",
      "P(High | No)  [feature=Humidity] = 4/5 = 0.800000\n",
      "P(Strong | No)  [feature=Wind] = 3/5 = 0.600000\n",
      "\n",
      "=== Naive Bayes unnormalized scores (no smoothing) ===\n",
      "score(Yes) = 0.0052910053  (≈ 0.0053)\n",
      "score(No)  = 0.0205714286  (≈ 0.0206)\n",
      "Prediction: No\n",
      "\n",
      "=== Log-scores (no smoothing) ===\n",
      "logscore(Yes) = -5.2417470151\n",
      "logscore(No)  = -3.8838521285\n",
      "Prediction (log): No\n",
      "\n",
      "=== Zero-count and Laplace smoothing example ===\n",
      "P(Overcast | No) without smoothing = 0/5 = 0.000000\n",
      "P(Overcast | No) with add-one smoothing = 1/8 = 0.125000\n",
      "\n",
      "Training accuracy (no smoothing, log): 0.929\n"
     ]
    }
   ],
   "source": [
    "# naive_bayes_solution.py\n",
    "# NumPy-only Naive Bayes (categorical) solution for the PlayTennis dataset.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_play_tennis_csv(path: str):\n",
    "    \"\"\"Loads play_tennis.csv into (X, y, feature_names). X is (N,4) strings, y is (N,) strings.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "\n",
    "    data = np.genfromtxt(path, delimiter=\",\", dtype=str, skip_header=1)\n",
    "    if data.ndim == 1:  # in case the CSV has only 1 row (not expected here)\n",
    "        data = data[None, :]\n",
    "\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    feature_names = header[:-1]\n",
    "    label_name = header[-1]\n",
    "    return X, y, feature_names, label_name\n",
    "\n",
    "\n",
    "def fit_naive_bayes_categorical(X: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Fit a categorical Naive Bayes model using frequency counts.\n",
    "    Stores priors, per-class counts, per-feature value sets, and conditional counts.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    classes, class_counts_arr = np.unique(y, return_counts=True)\n",
    "\n",
    "    class_counts = {c: int(cnt) for c, cnt in zip(classes, class_counts_arr)}\n",
    "    priors = {c: class_counts[c] / n_samples for c in classes}\n",
    "\n",
    "    feature_values = [np.unique(X[:, j]) for j in range(n_features)]\n",
    "\n",
    "    # Conditional counts: cond_counts[c][j][v] = count of (X_j=v AND y=c)\n",
    "    cond_counts = {c: [] for c in classes}\n",
    "    for c in classes:\n",
    "        mask_c = (y == c)\n",
    "        for j in range(n_features):\n",
    "            vals = feature_values[j]\n",
    "            counts_j = {v: int(np.sum(mask_c & (X[:, j] == v))) for v in vals}\n",
    "            cond_counts[c].append(counts_j)\n",
    "\n",
    "    model = {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"n_features\": n_features,\n",
    "        \"classes\": classes,\n",
    "        \"class_counts\": class_counts,\n",
    "        \"priors\": priors,\n",
    "        \"feature_values\": feature_values,\n",
    "        \"cond_counts\": cond_counts,\n",
    "    }\n",
    "    return model\n",
    "\n",
    "\n",
    "def conditional_prob(model, class_label: str, feature_index: int, feature_value: str, smoothing: bool = False):\n",
    "    \"\"\"\n",
    "    Returns P(X_j=feature_value | y=class_label).\n",
    "    If smoothing=False: frequency estimate.\n",
    "    If smoothing=True: add-one smoothing with K_j = #unique values in feature j.\n",
    "    Handles unseen feature_value at prediction time.\n",
    "    \"\"\"\n",
    "    n_c = model[\"class_counts\"][class_label]\n",
    "    K_j = len(model[\"feature_values\"][feature_index])\n",
    "\n",
    "    count = model[\"cond_counts\"][class_label][feature_index].get(feature_value, 0)\n",
    "\n",
    "    if not smoothing:\n",
    "        return count / n_c if n_c > 0 else 0.0\n",
    "\n",
    "    # Add-one (Laplace) smoothing\n",
    "    return (count + 1) / (n_c + K_j)\n",
    "\n",
    "\n",
    "def conditional_fraction(model, class_label: str, feature_index: int, feature_value: str, smoothing: bool = False):\n",
    "    \"\"\"\n",
    "    Returns (numerator, denominator) for P(X_j=feature_value | y=class_label),\n",
    "    matching the slide-style fractions.\n",
    "    \"\"\"\n",
    "    n_c = model[\"class_counts\"][class_label]\n",
    "    K_j = len(model[\"feature_values\"][feature_index])\n",
    "\n",
    "    count = model[\"cond_counts\"][class_label][feature_index].get(feature_value, 0)\n",
    "\n",
    "    if not smoothing:\n",
    "        return count, n_c\n",
    "    return count + 1, n_c + K_j\n",
    "\n",
    "\n",
    "def prior_fraction(model, class_label: str):\n",
    "    \"\"\"Returns (numerator, denominator) for P(y=class_label).\"\"\"\n",
    "    return model[\"class_counts\"][class_label], model[\"n_samples\"]\n",
    "\n",
    "\n",
    "def score_naive_bayes(model, x_star: np.ndarray, class_label: str, smoothing: bool = False, use_log: bool = False):\n",
    "    \"\"\"\n",
    "    Unnormalized NB score: P(c) * Π_j P(x_j | c)\n",
    "    If use_log=True: returns log-score to avoid underflow.\n",
    "    \"\"\"\n",
    "    prior = model[\"priors\"][class_label]\n",
    "\n",
    "    if use_log:\n",
    "        s = np.log(prior)\n",
    "        for j, v in enumerate(x_star):\n",
    "            p = conditional_prob(model, class_label, j, v, smoothing=smoothing)\n",
    "            if p <= 0.0:\n",
    "                return -np.inf\n",
    "            s += np.log(p)\n",
    "        return s\n",
    "\n",
    "    s = prior\n",
    "    for j, v in enumerate(x_star):\n",
    "        p = conditional_prob(model, class_label, j, v, smoothing=smoothing)\n",
    "        s *= p\n",
    "    return s\n",
    "\n",
    "\n",
    "def predict_one(model, x_star: np.ndarray, smoothing: bool = False, use_log: bool = False):\n",
    "    \"\"\"Predict label for one sample x_star.\"\"\"\n",
    "    classes = model[\"classes\"]\n",
    "    scores = {c: score_naive_bayes(model, x_star, c, smoothing=smoothing, use_log=use_log) for c in classes}\n",
    "    pred = max(scores, key=scores.get)\n",
    "    return pred, scores\n",
    "\n",
    "\n",
    "def predict(model, X: np.ndarray, smoothing: bool = False, use_log: bool = False):\n",
    "    \"\"\"Predict labels for a batch X.\"\"\"\n",
    "    preds = []\n",
    "    for i in range(X.shape[0]):\n",
    "        pred_i, _ = predict_one(model, X[i], smoothing=smoothing, use_log=use_log)\n",
    "        preds.append(pred_i)\n",
    "    return np.array(preds, dtype=str)\n",
    "\n",
    "\n",
    "def fmt_frac(num: int, den: int):\n",
    "    return f\"{num}/{den}\"\n",
    "\n",
    "\n",
    "def print_slide_example(model, feature_names):\n",
    "    \"\"\"\n",
    "    Reproduce the slide’s intermediate probabilities and NB scores for:\n",
    "    x* = (Sunny, Cool, High, Strong)\n",
    "    \"\"\"\n",
    "    x_star = np.array([\"Sunny\", \"Cool\", \"High\", \"Strong\"], dtype=str)\n",
    "\n",
    "    print(\"=== Slide-style intermediate probabilities (no smoothing) ===\")\n",
    "    print(\"x* =\", x_star.tolist())\n",
    "    print()\n",
    "\n",
    "    # Priors\n",
    "    for c in [\"Yes\", \"No\"]:\n",
    "        num, den = prior_fraction(model, c)\n",
    "        print(f\"P({c}) = {fmt_frac(num, den)} = {num/den:.6f}\")\n",
    "    print()\n",
    "\n",
    "    # Conditionals\n",
    "    for c in [\"Yes\", \"No\"]:\n",
    "        print(f\"--- Conditionals given {c} ---\")\n",
    "        for j, v in enumerate(x_star):\n",
    "            num, den = conditional_fraction(model, c, j, v, smoothing=False)\n",
    "            p = num / den if den > 0 else 0.0\n",
    "            print(f\"P({v} | {c})  [feature={feature_names[j]}] = {fmt_frac(num, den)} = {p:.6f}\")\n",
    "        print()\n",
    "\n",
    "    # Scores (as in slide)\n",
    "    score_yes = score_naive_bayes(model, x_star, \"Yes\", smoothing=False, use_log=False)\n",
    "    score_no = score_naive_bayes(model, x_star, \"No\", smoothing=False, use_log=False)\n",
    "\n",
    "    print(\"=== Naive Bayes unnormalized scores (no smoothing) ===\")\n",
    "    print(f\"score(Yes) = {score_yes:.10f}  (≈ {score_yes:.4f})\")\n",
    "    print(f\"score(No)  = {score_no:.10f}  (≈ {score_no:.4f})\")\n",
    "    print(\"Prediction:\", \"Yes\" if score_yes > score_no else \"No\")\n",
    "    print()\n",
    "\n",
    "    # Also show log-scores\n",
    "    log_yes = score_naive_bayes(model, x_star, \"Yes\", smoothing=False, use_log=True)\n",
    "    log_no = score_naive_bayes(model, x_star, \"No\", smoothing=False, use_log=True)\n",
    "    print(\"=== Log-scores (no smoothing) ===\")\n",
    "    print(f\"logscore(Yes) = {log_yes:.10f}\")\n",
    "    print(f\"logscore(No)  = {log_no:.10f}\")\n",
    "    print(\"Prediction (log):\", \"Yes\" if log_yes > log_no else \"No\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_zero_count_example(model):\n",
    "    \"\"\"\n",
    "    Demonstrate the zero-count problem and Laplace smoothing for:\n",
    "    P(Overcast | No)\n",
    "    \"\"\"\n",
    "    j_outlook = 0\n",
    "    v = \"Overcast\"\n",
    "    c = \"No\"\n",
    "\n",
    "    num0, den0 = conditional_fraction(model, c, j_outlook, v, smoothing=False)\n",
    "    p0 = conditional_prob(model, c, j_outlook, v, smoothing=False)\n",
    "\n",
    "    num1, den1 = conditional_fraction(model, c, j_outlook, v, smoothing=True)\n",
    "    p1 = conditional_prob(model, c, j_outlook, v, smoothing=True)\n",
    "\n",
    "    print(\"=== Zero-count and Laplace smoothing example ===\")\n",
    "    print(f\"P({v} | {c}) without smoothing = {fmt_frac(num0, den0)} = {p0:.6f}\")\n",
    "    print(f\"P({v} | {c}) with add-one smoothing = {fmt_frac(num1, den1)} = {p1:.6f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Adjust path if needed\n",
    "    X, y, feature_names, label_name = load_play_tennis_csv(\"play_tennis.csv\")\n",
    "\n",
    "    print(f\"Loaded dataset with N={X.shape[0]} rows, {X.shape[1]} features. Label column: {label_name}\")\n",
    "    print(\"Feature names:\", feature_names)\n",
    "    print(\"Class labels:\", np.unique(y).tolist())\n",
    "    print()\n",
    "\n",
    "    model = fit_naive_bayes_categorical(X, y)\n",
    "\n",
    "    # Reproduce the lecture-slide example\n",
    "    print_slide_example(model, feature_names)\n",
    "\n",
    "    # Show zero-count and smoothing example\n",
    "    print_zero_count_example(model)\n",
    "\n",
    "    # (Optional) predict all training samples (not required, but useful sanity check)\n",
    "    preds = predict(model, X, smoothing=False, use_log=True)\n",
    "    acc = np.mean(preds == y)\n",
    "    print(\"Training accuracy (no smoothing, log):\", f\"{acc:.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
